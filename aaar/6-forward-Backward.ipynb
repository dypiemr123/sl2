{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae93771-da36-476d-a52d-54d5ffe37e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name: Aarya Admane\n",
    "Rollno:22630\n",
    "Div:B Batch:B2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d18495-e335-40a6-9974-9ef2c55de8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda86f6e-330e-403a-b11e-b6bee9586a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098bda36-1f1e-4a83-ae95-caecb8cfb01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of sigmoid for backpropagation\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c3321b-9989-4f2a-8f27-ef971fee2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error Loss function\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "234221f8-cbfd-4049-80ca-5d0b906e4537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize neural network parameters\n",
    "np.random.seed(42)\n",
    "input_size = 2   # Number of input neurons\n",
    "hidden_size = 3  # Number of neurons in hidden layer\n",
    "output_size = 1  # Number of output neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9555105-f235-4b13-a705-3e2598a3235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize weights and biases randomly\n",
    "W1 = np.random.randn(input_size, hidden_size)  # Input to hidden layer weights\n",
    "b1 = np.random.randn(1, hidden_size)           # Bias for hidden layer\n",
    "W2 = np.random.randn(hidden_size, output_size) # Hidden to output weights\n",
    "b2 = np.random.randn(1, output_size)           # Bias for output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49a48e29-e56d-450e-8f3f-2971bf53d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training data (XOR problem)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
    "y = np.array([[0], [1], [1], [0]])             # Expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2485c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71e24123-9de8-4fdf-aa7e-5f76eed54672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training parameters\n",
    "epochs = 10000  # Number of training iterations\n",
    "learning_rate = 0.1  # Learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fe35060-f922-4e9a-8cae-bfcb15d1b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward Propagation\n",
    "    z1 = np.dot(X, W1) + b1   # Linear transformation (Input → Hidden)\n",
    "    a1 = sigmoid(z1)          # Activation function (Hidden layer)\n",
    "    z2 = np.dot(a1, W2) + b2  # Linear transformation (Hidden → Output)\n",
    "    a2 = sigmoid(z2)          # Activation function (Output layer)\n",
    "\n",
    "  # Compute loss\n",
    "    loss = mse_loss(y, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0379cca4-0efa-4205-86e5-e261beeb0024",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Backpropagation (Calculating Gradients)\n",
    "\n",
    "d_a2 = (a2 - y)  # Loss gradient\n",
    "d_z2 = d_a2 * sigmoid_derivative(a2)  # Backprop through activation\n",
    "d_W2 = np.dot(a1.T, d_z2)  # Weight gradient (hidden → output)\n",
    "d_b2 = np.sum(d_z2, axis=0, keepdims=True)  # Bias gradient (output)\n",
    "\n",
    "d_a1 = np.dot(d_z2, W2.T)  # Backprop to hidden layer\n",
    "d_z1 = d_a1 * sigmoid_derivative(a1)  # Backprop through activation\n",
    "d_W1 = np.dot(X.T, d_z1)  # Weight gradient (input → hidden)\n",
    "d_b1 = np.sum(d_z1, axis=0, keepdims=True)  # Bias gradient (hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43684376-8ba1-4704-9c54-5adc57284203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Update weights and biases\n",
    "    W2 -= learning_rate * d_W2\n",
    "    b2 -= learning_rate * d_b2\n",
    "    W1 -= learning_rate * d_W1\n",
    "    b1 -= learning_rate * d_b1\n",
    "\n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dda4b30-fa35-49e6-99d3-1420378958b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Predictions:\n",
      "[[0.54553169]\n",
      " [0.57471011]\n",
      " [0.53845304]\n",
      " [0.56280605]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Final Predictions\n",
    "print(\"\\nFinal Predictions:\")\n",
    "print(sigmoid(np.dot(sigmoid(np.dot(X, W1) + b1), W2) + b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7414d71c-ac2f-43a3-b83e-741d77b0538e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
